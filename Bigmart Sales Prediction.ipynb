{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigMart Sales Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store. \n",
    "\n",
    "Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's categorize our hypothesis into following 2 categories: -\n",
    "\n",
    "<b> Store Level Hypotheses:\n",
    "1. City type: Stores located in urban or Tier 1 cities should have higher sales because of the higher income levels of people there.\n",
    "2. Population Density: Stores located in densely populated areas should have higher sales because of more demand.\n",
    "3. Store Capacity: Stores which are very big in size should have higher sales as they act like one-stop-shops and people would prefer getting everything from one place.\n",
    "4. Competitors: Stores have similar establishments nearby should have less sales because of more competition.\n",
    "5. Marketing: Stores which have a good marketing division should have higher sales as it will be able to attract customers through the right offers and advertising.\n",
    "6. Location: Stores located within popular marketplaces should have higher sales because of better access to customers.\n",
    "7. Customer Behavior: Stores keeping the right set of products to meet the local needs of customers will have higher sales.\n",
    "8. Ambiance: Stores which are well-maintained and managed by polite and humble people are expected to have higher footfall and thus higher sales.\n",
    "\n",
    "<b> Product Level Hypotheses:\n",
    "1. Brand: Branded products should have higher sales because of higher trust in the customer.\n",
    "2. Packaging: Products with good packaging can attract customers and sell more.\n",
    "3. Utility: Daily use products should have a higher tendency to sell as compared to the specific use products.\n",
    "4. Display Area: Products which are given bigger shelves in the store are likely to catch attention first and sell more.\n",
    "5. Visibility in Store: The location of product in a store will impact sales. Ones which are right at entrance will catch the eye of customer first rather than the ones in back.\n",
    "6. Advertising: Better advertising of products in the store should higher sales in most cases.\n",
    "7. Promotional Offers: Products accompanied with attractive offers and discounts will sell more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be performing some basic data exploration here and come up with some interences about the data.\n",
    "\n",
    "Let's import libraries and load train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Read files:\n",
    "train=pd.read_csv(\"Train.csv\")\n",
    "test=pd.read_csv(\"Test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its generally a good idea to combine both train and test data sets into one, perform feature engineering and then divide them later again. This saves the trouble of performing the same steps twice on test and train. Lets combine them into a dataframe 'data' with a 'source' column specifying where each observation belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8523, 13) (5681, 12) (14204, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyesh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "train['source']='train'\n",
    "test['source']='test'\n",
    "data=pd.concat([train, test], ignore_index=True)\n",
    "print (train.shape, test.shape, data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see that data has same number of columns but rows equivalent to both test and train. One of the key challenges in any data set is missing values. Lets start by checking which columns contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Fat_Content                0\n",
       "Item_Identifier                 0\n",
       "Item_MRP                        0\n",
       "Item_Outlet_Sales            5681\n",
       "Item_Type                       0\n",
       "Item_Visibility                 0\n",
       "Item_Weight                  2439\n",
       "Outlet_Establishment_Year       0\n",
       "Outlet_Identifier               0\n",
       "Outlet_Location_Type            0\n",
       "Outlet_Size                  4016\n",
       "Outlet_Type                     0\n",
       "source                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Item_Outlet_Sales is the target variable and missing values are ones in the test set. So we need not worry about it. But we'll impute the missing values in Item_Weight and Outlet_Size in the data cleaning section.\n",
    "\n",
    "Lets look at some basic statistics for numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14204.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "      <td>14204.000000</td>\n",
       "      <td>11765.000000</td>\n",
       "      <td>14204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>141.004977</td>\n",
       "      <td>2181.288914</td>\n",
       "      <td>0.065953</td>\n",
       "      <td>12.792854</td>\n",
       "      <td>1997.830681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>62.086938</td>\n",
       "      <td>1706.499616</td>\n",
       "      <td>0.051459</td>\n",
       "      <td>4.652502</td>\n",
       "      <td>8.371664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>31.290000</td>\n",
       "      <td>33.290000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.555000</td>\n",
       "      <td>1985.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>94.012000</td>\n",
       "      <td>834.247400</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>8.710000</td>\n",
       "      <td>1987.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>142.247000</td>\n",
       "      <td>1794.331000</td>\n",
       "      <td>0.054021</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>1999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>185.855600</td>\n",
       "      <td>3101.296400</td>\n",
       "      <td>0.094037</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>2004.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>266.888400</td>\n",
       "      <td>13086.964800</td>\n",
       "      <td>0.328391</td>\n",
       "      <td>21.350000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Item_MRP  Item_Outlet_Sales  Item_Visibility   Item_Weight  \\\n",
       "count  14204.000000        8523.000000     14204.000000  11765.000000   \n",
       "mean     141.004977        2181.288914         0.065953     12.792854   \n",
       "std       62.086938        1706.499616         0.051459      4.652502   \n",
       "min       31.290000          33.290000         0.000000      4.555000   \n",
       "25%       94.012000         834.247400         0.027036      8.710000   \n",
       "50%      142.247000        1794.331000         0.054021     12.600000   \n",
       "75%      185.855600        3101.296400         0.094037     16.750000   \n",
       "max      266.888400       13086.964800         0.328391     21.350000   \n",
       "\n",
       "       Outlet_Establishment_Year  \n",
       "count               14204.000000  \n",
       "mean                 1997.830681  \n",
       "std                     8.371664  \n",
       "min                  1985.000000  \n",
       "25%                  1987.000000  \n",
       "50%                  1999.000000  \n",
       "75%                  2004.000000  \n",
       "max                  2009.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "1. <b>Item_Visibility</b> has a min value of zero. This makes no practical sense because when a product is being sold in a store, the visibility cannot be 0.\n",
    "2. <b>Outlet_Establishment_Years</b> vary from 1985 to 2009. The values might not be apt in this form. Rather, if we can convert them to how old the particular store is, it should have a better impact on sales.\n",
    "3. The lower 'count' of Item_Weight and Item_Outlet_Sales confirms the findings from the missing value check.\n",
    "\n",
    "Moving to nominal(categorical) variable, lets have a look at the number of unique values in each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Fat_Content                 5\n",
       "Item_Identifier               1559\n",
       "Item_MRP                      8052\n",
       "Item_Outlet_Sales             3494\n",
       "Item_Type                       16\n",
       "Item_Visibility              13006\n",
       "Item_Weight                    416\n",
       "Outlet_Establishment_Year        9\n",
       "Outlet_Identifier               10\n",
       "Outlet_Location_Type             3\n",
       "Outlet_Size                      4\n",
       "Outlet_Type                      4\n",
       "source                           2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that there are <b>1559 products and 10 outlets/stores</b> (which was also mentioned in problem statement). Another thing that should catch attention is that <b>Item_Type has 16 unique values</b>. Let's explore further using the frequency of different categories in each nominal variable. I'll exclude the ID and source variables for obvious reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequency of Categories for variable  Item_Fat_Content\n",
      "Low Fat    8485\n",
      "Regular    4824\n",
      "LF          522\n",
      "reg         195\n",
      "low fat     178\n",
      "Name: Item_Fat_Content, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable  Item_Type\n",
      "Fruits and Vegetables    2013\n",
      "Snack Foods              1989\n",
      "Household                1548\n",
      "Frozen Foods             1426\n",
      "Dairy                    1136\n",
      "Baking Goods             1086\n",
      "Canned                   1084\n",
      "Health and Hygiene        858\n",
      "Meat                      736\n",
      "Soft Drinks               726\n",
      "Breads                    416\n",
      "Hard Drinks               362\n",
      "Others                    280\n",
      "Starchy Foods             269\n",
      "Breakfast                 186\n",
      "Seafood                    89\n",
      "Name: Item_Type, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable  Outlet_Location_Type\n",
      "Tier 3    5583\n",
      "Tier 2    4641\n",
      "Tier 1    3980\n",
      "Name: Outlet_Location_Type, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable  Outlet_Size\n",
      "Medium    4655\n",
      "Small     3980\n",
      "High      1553\n",
      "Name: Outlet_Size, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable  Outlet_Type\n",
      "Supermarket Type1    9294\n",
      "Grocery Store        1805\n",
      "Supermarket Type3    1559\n",
      "Supermarket Type2    1546\n",
      "Name: Outlet_Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Filter categorical variables\n",
    "\n",
    "categorical_columns= [x for x in data.dtypes.index if data.dtypes[x]=='object']\n",
    "\n",
    "#Exclude ID cols and source:\n",
    "categorical_columns= [x for x in categorical_columns if x not in ['Item_Identifier', 'Outlet_Identifier','source']]\n",
    "\n",
    "#Print frequency of categories\n",
    "for col in categorical_columns:\n",
    "    print('\\nFrequency of Categories for variable ',col)\n",
    "    print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output gives us following observations:\n",
    "1. <b>Item_Fat_Content:</b> Some of 'Low Fat' values mis-coded as 'low fat' and 'LF'.Also, some of 'Regular' are mentioned as 'regular'.\n",
    "2. <b>Item_Type:</b> Not all categories have substantial numbers. It looks like combining them can give better results.\n",
    "3. <b>Outlet_Type:</b> Supermarket Type2 and Type3 can be combined. But we should check if that's a good idea before doing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step typically involves imputing missing values and treating outliers. Though outlier removal is very important in regression techniques, advanced tree based algorithms are impervious to outliers. So I'll leave it to you to try it out. We'll focus on the imputation step here, which is a very important step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Imputing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found two variables with missing values - Item_Weight and Outlet_Size. Lets impute the former by the average weight of the particular item. This can be done as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original #missing:  2439\n",
      "Final #missing:  0\n"
     ]
    }
   ],
   "source": [
    "#Determine the average weight per item:\n",
    "item_avg_weight= data.pivot_table(values='Item_Weight', index='Item_Identifier')\n",
    "\n",
    "#Get a boolean variable specifying missing Item_Weight values\n",
    "miss_bool=data['Item_Weight'].isnull()\n",
    "\n",
    "#Impute data and check #missing values before and after imputation to confirm \n",
    "print(\"Original #missing: \", sum(miss_bool))\n",
    "data.loc[miss_bool,'Item_Weight'] = data.loc[miss_bool,'Item_Identifier'].apply(lambda x: item_avg_weight.loc[x])\n",
    "print(\"Final #missing: \", sum(data['Item_Weight'].isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that the column has no missing values now. Lets impute Outlet_Size with the mode of the Outlet_Size for the particular type of outlet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyesh/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode for each Outlet_Type:\n",
      "Outlet_Type Grocery Store Supermarket Type1 Supermarket Type2  \\\n",
      "Outlet_Size         Small             Small            Medium   \n",
      "\n",
      "Outlet_Type Supermarket Type3  \n",
      "Outlet_Size            Medium  \n",
      "\n",
      " Original #missing :  4016\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#impofg mode function:\n",
    "from scipy.stats import mode\n",
    "\n",
    "#Determining the mode for each\n",
    "outlet_size_mode=data.pivot_table(values='Outlet_Size', columns='Outlet_Type', aggfunc=(lambda x:mode(x.dropna()).mode[0]))\n",
    "print('Mode for each Outlet_Type:')\n",
    "print(outlet_size_mode)\n",
    "\n",
    "#Get a boolean variable specifying missing Item_Weight values\n",
    "miss_bool=data['Outlet_Size'].isnull()\n",
    "\n",
    "#Impute data and check #missing values before and after imputation to confirm\n",
    "print(\"\\n Original #missing : \", sum(miss_bool))\n",
    "\n",
    "data.loc[miss_bool,'Outlet_Size']=data.loc[miss_bool,'Outlet_Type'].apply(lambda x:outlet_size_mode[x])\n",
    "print(sum(data['Outlet_Size'].isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that there are no missing values in the data. Let's move on to feature engineering now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explored some nuances in the data in the data exploration section. Lets move on to resolving them and making our data ready for analysis. We will also create some new variables using the existing ones in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> Step 1: Consider combining Outlet_Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During exploration, we decided to consider combining the Supermarket Type2 and Type3 variables. But is that a good idea? A quick way to check that could be to analyze the mean sales by type of store. If they have similar sales, then keeping them separate won't help much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Grocery Store</th>\n",
       "      <td>339.828500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Supermarket Type1</th>\n",
       "      <td>2316.181148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Supermarket Type2</th>\n",
       "      <td>1995.498739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Supermarket Type3</th>\n",
       "      <td>3694.038558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Item_Outlet_Sales\n",
       "Outlet_Type                         \n",
       "Grocery Store             339.828500\n",
       "Supermarket Type1        2316.181148\n",
       "Supermarket Type2        1995.498739\n",
       "Supermarket Type3        3694.038558"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.pivot_table(values='Item_Outlet_Sales', index='Outlet_Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows significant difference between them and we'll leave them as it is. Note that just one way of doing this, you can perform some other analysis in different situations and also do the same for other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b> Step 2: Modify Item_Visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that the minimum value here is 0, which makes no practical sense. Lets consider it like missing information and impute it with mean visibility of that product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 0 values initially:  879\n",
      "Number of 0 values after modification 0\n"
     ]
    }
   ],
   "source": [
    "#Determine average visibility of a product\n",
    "visibility_avg=data.pivot_table(values='Item_Visibility', index='Item_Identifier')\n",
    "\n",
    "#Impute 0 values with mean visibility of that product:\n",
    "miss_bool= (data['Item_Visibility']==0)\n",
    "\n",
    "print(\"Number of 0 values initially: \",sum(miss_bool))\n",
    "\n",
    "data.loc[miss_bool,'Item_Visibility']=data.loc[miss_bool,'Item_Identifier'].apply(lambda x:visibility_avg.loc[x])\n",
    "print('Number of 0 values after modification',sum(data['Item_Visibility']==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there are no values which are zero.\n",
    "\n",
    "In step 1 we hypothesized that products with higher visibility are likely to sell more. But along with comparing products on absolute terms, we should look at the visbility of the product in that particular store as compared to the mean visbility of that product across all stores. This will give some idea about how much importance was given to that product in a store as compared to other stoores. We can use the 'visbility_avg' variable made above to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    14204.000000\n",
      "mean         1.061884\n",
      "std          0.235907\n",
      "min          0.844563\n",
      "25%          0.925131\n",
      "50%          0.999070\n",
      "75%          1.042007\n",
      "max          3.010094\n",
      "Name: Item_Visibility_MeanRatio, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Determine another variable with means ratio\n",
    "data['Item_Visibility_MeanRatio']=data.apply(lambda x:x.loc['Item_Visibility']/visibility_avg.loc[x['Item_Identifier']], axis=1)\n",
    "print(data['Item_Visibility_MeanRatio'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the new variable has been successfully created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step 3: Create a broad category of Type of Item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we saw that the Item_Type variable has 16 categories which might prove to be very useful in analysis. So its a good idea to combine them. One way could be to manually assign a new category to each. But there's a catch here. If you look at the Item_Identifier, i.e. the unique ID of each item, it starts with either FD, DR or NC. If you see the categories, these look like being Food, Drinks and Non-Consumables. So I've used the Item_Identifier variable to create a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Food              10201\n",
       "Non-Consumable     2686\n",
       "Drinks             1317\n",
       "Name: Item_Type_Combined, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the first two characters of ID:\n",
    "data['Item_Type_Combined']=data['Item_Identifier'].apply(lambda x: x[0:2])\n",
    "\n",
    "#Rename them to more intuitive categories:\n",
    "data['Item_Type_Combined']=data['Item_Type_Combined'].map({'FD':'Food','NC':'Non-Consumable','DR':'Drinks'})\n",
    "data['Item_Type_Combined'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step 4: Determine the years of operation of a store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to make a new column depicting the years of operation of a store. This can be done as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14204.000000\n",
       "mean        15.169319\n",
       "std          8.371664\n",
       "min          4.000000\n",
       "25%          9.000000\n",
       "50%         14.000000\n",
       "75%         26.000000\n",
       "max         28.000000\n",
       "Name: Outlet_Years, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Years:\n",
    "data['Outlet_Years']=2013-data['Outlet_Establishment_Year']\n",
    "data['Outlet_Years'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows stores which are 4-28 years old. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step 5: Modify categories of Item_Fat_Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found typos and difference in representation in categories of Item_Fat_Content variable. This can be corrected as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Categories\n",
      "Low Fat    8485\n",
      "Regular    4824\n",
      "LF          522\n",
      "reg         195\n",
      "low fat     178\n",
      "Name: Item_Fat_Content, dtype: int64\n",
      "\n",
      "Modified Categories:\n",
      "Low Fat    9185\n",
      "Regular    5019\n",
      "Name: Item_Fat_Content, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Change categories of low fat:\n",
    "print(\"Original Categories\")\n",
    "print(data['Item_Fat_Content'].value_counts())\n",
    "print(\"\\nModified Categories:\")\n",
    "data['Item_Fat_Content']=data['Item_Fat_Content'].replace({'LF':'Low Fat', 'reg':'Regular','low fat':'Low Fat'})\n",
    "print(data['Item_Fat_Content'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it makes more sense. But hang on, in step 4 we saw there were some non-consumables as well and a fat-content should not be specified for them. So we can also create a separate category for such kind of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Low Fat       6499\n",
       "Regular       5019\n",
       "Non-Edible    2686\n",
       "Name: Item_Fat_Content, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mark non-consumables as separate category in low_fat:\n",
    "data.loc[data['Item_Type_Combined']==\"Non-Consumable\",\"Item_Fat_Content\"]=\"Non-Edible\"\n",
    "data['Item_Fat_Content'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step 6: Numerical and One-Hot Coding of Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since scikit-learn accepts only numerical variables, I converted all categories of nominal variables into numeric types. Also, I wanted Outlet_Identifier as a variable as well. So I created a new variable 'Outlet' same as Outlet_Identifier and coded that. Outlet_Identifier should remain as it is, because it will be required in the submission file.\n",
    "\n",
    "Let's start with coding all categorical variables as numeric using 'LabelEncoder' from sklearn's preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "\n",
    "#New variable for outlet\n",
    "data['Outlet']=le.fit_transform(data['Outlet_Identifier'])\n",
    "var_mod=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet']\n",
    "le=LabelEncoder()\n",
    "for i in var_mod:\n",
    "    data[i]=le.fit_transform(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot-Coding refers to creatng dummy variables, one for each category of a categorical variable. For example, the Item_Fat_Content has 3 categories- 'Low Fat', 'Regular' and 'Non-Edible'. One hot coding will remove this variable and generate 3 new variables. Each will have binary numbers - 0 (if the category is not present) and 1 (if category is present). This can be done using 'get_dummies' function of Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Coding:\n",
    "data=pd.get_dummies(data, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type','Item_Type_Combined','Outlet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the datatypes of columns now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier               object\n",
       "Item_MRP                     float64\n",
       "Item_Outlet_Sales            float64\n",
       "Item_Type                     object\n",
       "Item_Visibility              float64\n",
       "Item_Weight                  float64\n",
       "Outlet_Establishment_Year      int64\n",
       "Outlet_Identifier             object\n",
       "source                        object\n",
       "Item_Visibility_MeanRatio    float64\n",
       "Outlet_Years                   int64\n",
       "Item_Fat_Content_0             uint8\n",
       "Item_Fat_Content_1             uint8\n",
       "Item_Fat_Content_2             uint8\n",
       "Outlet_Location_Type_0         uint8\n",
       "Outlet_Location_Type_1         uint8\n",
       "Outlet_Location_Type_2         uint8\n",
       "Outlet_Size_0                  uint8\n",
       "Outlet_Size_1                  uint8\n",
       "Outlet_Size_2                  uint8\n",
       "Outlet_Type_0                  uint8\n",
       "Outlet_Type_1                  uint8\n",
       "Outlet_Type_2                  uint8\n",
       "Outlet_Type_3                  uint8\n",
       "Item_Type_Combined_0           uint8\n",
       "Item_Type_Combined_1           uint8\n",
       "Item_Type_Combined_2           uint8\n",
       "Outlet_0                       uint8\n",
       "Outlet_1                       uint8\n",
       "Outlet_2                       uint8\n",
       "Outlet_3                       uint8\n",
       "Outlet_4                       uint8\n",
       "Outlet_5                       uint8\n",
       "Outlet_6                       uint8\n",
       "Outlet_7                       uint8\n",
       "Outlet_8                       uint8\n",
       "Outlet_9                       uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see variables are either float or integer. Let look at the 3 columns formed from Item_Fat_Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Fat_Content_0</th>\n",
       "      <th>Item_Fat_Content_1</th>\n",
       "      <th>Item_Fat_Content_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Item_Fat_Content_0  Item_Fat_Content_1  Item_Fat_Content_2\n",
       "0                   1                   0                   0\n",
       "1                   0                   0                   1\n",
       "2                   1                   0                   0\n",
       "3                   0                   0                   1\n",
       "4                   0                   1                   0\n",
       "5                   0                   0                   1\n",
       "6                   0                   0                   1\n",
       "7                   1                   0                   0\n",
       "8                   0                   0                   1\n",
       "9                   0                   0                   1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Item_Fat_Content_0','Item_Fat_Content_1','Item_Fat_Content_2']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that each row have one of the columns as 1 corresponding to the category in the original variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Step 7: Exporting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step is to convert data back into train and test data sets. Its generally a good idea to export both of these as modified data sets so that they can be re-used for multiple sessions. This can be achieved using following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyesh/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "#Drop the columns which have been converted to different types:\n",
    "data.drop(['Item_Type','Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "\n",
    "#Divide into test and train:\n",
    "train=data.loc[data['source']==\"train\"]\n",
    "test=data.loc[data['source']==\"test\"]\n",
    "\n",
    "#Drop unnecessary columns:\n",
    "test.drop(['Item_Outlet_Sales','source'], axis=1, inplace=True)\n",
    "train.drop(['source'], axis=1, inplace=True)\n",
    "\n",
    "#Export files as modified versions:\n",
    "train.to_csv(\"train_modified.csv\",index=False)\n",
    "test.to_csv(\"test_modified.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data ready, its time to start making predictive models. I will take you through 6 models including linear regression, decision tree and random forest.\n",
    "\n",
    "Lets start by making a baseline model. Baseline model is the one which requires no predictive model and its like an informed guess. For instance, in this case lets predict the sales as the overall average sales. This can be done as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyesh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Mean based:\n",
    "mean_sales=train['Item_Outlet_Sales'].mean()\n",
    "\n",
    "#Define a dataframe with IDs for submission:\n",
    "base1=test[['Item_Identifier','Outlet_Identifier']]\n",
    "base1.loc['Item_Outlet_Sales']=mean_sales\n",
    "\n",
    "#Export submission file\n",
    "base1.to_csv(\"a1g0.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make many models, instead of repeating the codes again and again, I would like to define a generic function which takes the algorithm and data as input and makes the model, performs cross-validation and generates submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and ID columns:\n",
    "target='Item_Outlet_Sales'\n",
    "IDcol=['Item_Identifier','Outlet_Identifier']\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def modelfit(alg, dtrain, dtest, predictors, target, IDcol, filename):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "    \n",
    "    #Predict training set\n",
    "    dtrain_predictions=alg.predict(dtrain[predictors])\n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    cv_score=cross_val_score(alg, dtrain[predictors], dtrain[target], cv=20, scoring='neg_mean_squared_error')\n",
    "    cv_score=np.sqrt(np.abs(cv_score))\n",
    "    \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"RMSE : \",np.sqrt(metrics.mean_squared_error(dtrain[target].values, dtrain_predictions)))\n",
    "    print(\"CV Score : Mean - %.4f | Std - %.4f | Min - %.4f | Max - %.4f\", (np.mean(cv_score), np.std(cv_score), np.min(cv_score), np.max(cv_score)) )\n",
    "    \n",
    "    #Predict on testing data:\n",
    "    dtest[target]= alg.predict(dtest[predictors])\n",
    "    \n",
    "    #Export submission file:\n",
    "    IDcol.append(target)\n",
    "    submission=pd.DataFrame({x:dtest[x] for x in IDcol})\n",
    "    submission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with making a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "RMSE :  1127.5940405931058\n",
      "CV Score : Mean - %.4f | Std - %.4f | Min - %.4f | Max - %.4f (1129.06319444549, 43.89748957663282, 1074.7241901202021, 1213.4791183795023)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyesh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff355976630>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "predictors=[x for x in train.columns if x not in [target]+IDcol]\n",
    "\n",
    "#print predictors\n",
    "alg1=LinearRegression(normalize=True)\n",
    "modelfit(alg1, train, test, predictors, target, IDcol, 'alg1.csv')\n",
    "coef1=pd.Series(alg1.coef_, predictors).sort_values()\n",
    "coef1.plot(kind='bar', title='Model_Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
