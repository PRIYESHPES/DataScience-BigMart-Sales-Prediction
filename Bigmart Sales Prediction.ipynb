{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigMart Sales Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store. \n",
    "\n",
    "Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's categorize our hypothesis into following 2 categories: -\n",
    "\n",
    "<b> Store Level Hypotheses:\n",
    "1. City type: Stores located in urban or Tier 1 cities should have higher sales because of the higher income levels of people there.\n",
    "2. Population Density: Stores located in densely populated areas should have higher sales because of more demand.\n",
    "3. Store Capacity: Stores which are very big in size should have higher sales as they act like one-stop-shops and people would prefer getting everything from one place.\n",
    "4. Competitors: Stores have similar establishments nearby should have less sales because of more competition.\n",
    "5. Marketing: Stores which have a good marketing division should have higher sales as it will be able to attract customers through the right offers and advertising.\n",
    "6. Location: Stores located within popular marketplaces should have higher sales because of better access to customers.\n",
    "7. Customer Behavior: Stores keeping the right set of products to meet the local needs of customers will have higher sales.\n",
    "8. Ambiance: Stores which are well-maintained and managed by polite and humble people are expected to have higher footfall and thus higher sales.\n",
    "\n",
    "<b> Product Level Hypotheses:\n",
    "1. Brand: Branded products should have higher sales because of higher trust in the customer.\n",
    "2. Packaging: Products with good packaging can attract customers and sell more.\n",
    "3. Utility: Daily use products should have a higher tendency to sell as compared to the specific use products.\n",
    "4. Display Area: Products which are given bigger shelves in the store are likely to catch attention first and sell more.\n",
    "5. Visibility in Store: The location of product in a store will impact sales. Ones which are right at entrance will catch the eye of customer first rather than the ones in back.\n",
    "6. Advertising: Better advertising of products in the store should higher sales in most cases.\n",
    "7. Promotional Offers: Products accompanied with attractive offers and discounts will sell more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be performing some basic data exploration here and come up with some interences about the data.\n",
    "\n",
    "Let's import libraries and load train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Read files:\n",
    "train=pd.read_csv(\"Train.csv\")\n",
    "test=pd.read_csv(\"Test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its generally a good idea to combine both train and test data sets into one, perform feature engineering and then divide them later again. This saves the trouble of performing the same steps twice on test and train. Lets combine them into a dataframe 'data' with a 'source' column specifying where each observation belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8523, 13) (5681, 12) (14204, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyesh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "train['source']='train'\n",
    "test['source']='test'\n",
    "data=pd.concat([train, test], ignore_index=True)\n",
    "print (train.shape, test.shape, data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see that data has same number of columns but rows equivalent to both test and train. One of the key challenges in any data set is missing values. Lets start by checking which columns contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Fat_Content                0\n",
       "Item_Identifier                 0\n",
       "Item_MRP                        0\n",
       "Item_Outlet_Sales            5681\n",
       "Item_Type                       0\n",
       "Item_Visibility                 0\n",
       "Item_Weight                  2439\n",
       "Outlet_Establishment_Year       0\n",
       "Outlet_Identifier               0\n",
       "Outlet_Location_Type            0\n",
       "Outlet_Size                  4016\n",
       "Outlet_Type                     0\n",
       "source                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Item_Outlet_Sales is the target variable and missing values are ones in the test set. So we need not worry about it. But we'll impute the missing values in Item_Weight and Outlet_Size in the data cleaning section.\n",
    "\n",
    "Lets look at some basic statistics for numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14204.000000</td>\n",
       "      <td>8523.000000</td>\n",
       "      <td>14204.000000</td>\n",
       "      <td>11765.000000</td>\n",
       "      <td>14204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>141.004977</td>\n",
       "      <td>2181.288914</td>\n",
       "      <td>0.065953</td>\n",
       "      <td>12.792854</td>\n",
       "      <td>1997.830681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>62.086938</td>\n",
       "      <td>1706.499616</td>\n",
       "      <td>0.051459</td>\n",
       "      <td>4.652502</td>\n",
       "      <td>8.371664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>31.290000</td>\n",
       "      <td>33.290000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.555000</td>\n",
       "      <td>1985.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>94.012000</td>\n",
       "      <td>834.247400</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>8.710000</td>\n",
       "      <td>1987.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>142.247000</td>\n",
       "      <td>1794.331000</td>\n",
       "      <td>0.054021</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>1999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>185.855600</td>\n",
       "      <td>3101.296400</td>\n",
       "      <td>0.094037</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>2004.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>266.888400</td>\n",
       "      <td>13086.964800</td>\n",
       "      <td>0.328391</td>\n",
       "      <td>21.350000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Item_MRP  Item_Outlet_Sales  Item_Visibility   Item_Weight  \\\n",
       "count  14204.000000        8523.000000     14204.000000  11765.000000   \n",
       "mean     141.004977        2181.288914         0.065953     12.792854   \n",
       "std       62.086938        1706.499616         0.051459      4.652502   \n",
       "min       31.290000          33.290000         0.000000      4.555000   \n",
       "25%       94.012000         834.247400         0.027036      8.710000   \n",
       "50%      142.247000        1794.331000         0.054021     12.600000   \n",
       "75%      185.855600        3101.296400         0.094037     16.750000   \n",
       "max      266.888400       13086.964800         0.328391     21.350000   \n",
       "\n",
       "       Outlet_Establishment_Year  \n",
       "count               14204.000000  \n",
       "mean                 1997.830681  \n",
       "std                     8.371664  \n",
       "min                  1985.000000  \n",
       "25%                  1987.000000  \n",
       "50%                  1999.000000  \n",
       "75%                  2004.000000  \n",
       "max                  2009.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "1. <b>Item_Visibility</b> has a min value of zero. This makes no practical sense because when a product is being sold in a store, the visibility cannot be 0.\n",
    "2. <b>Outlet_Establishment_Years</b> vary from 1985 to 2009. The values might not be apt in this form. Rather, if we can convert them to how old the particular store is, it should have a better impact on sales.\n",
    "3. The lower 'count' of Item_Weight and Item_Outlet_Sales confirms the findings from the missing value check.\n",
    "\n",
    "Moving to nominal(categorical) variable, lets have a look at the number of unique values in each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Fat_Content                 5\n",
       "Item_Identifier               1559\n",
       "Item_MRP                      8052\n",
       "Item_Outlet_Sales             3494\n",
       "Item_Type                       16\n",
       "Item_Visibility              13006\n",
       "Item_Weight                    416\n",
       "Outlet_Establishment_Year        9\n",
       "Outlet_Identifier               10\n",
       "Outlet_Location_Type             3\n",
       "Outlet_Size                      4\n",
       "Outlet_Type                      4\n",
       "source                           2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that there are <b>1559 products and 10 outlets/stores</b> (which was also mentioned in problem statement). Another thing that should catch attention is that <b>Item_Type has 16 unique values</b>. Let's explore further using the frequency of different categories in each nominal variable. I'll exclude the ID and source variables for obvious reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequency of Categories for variable  Item_Fat_Content\n",
      "Low Fat    8485\n",
      "Regular    4824\n",
      "LF          522\n",
      "reg         195\n",
      "low fat     178\n",
      "Name: Item_Fat_Content, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable  Item_Type\n",
      "Fruits and Vegetables    2013\n",
      "Snack Foods              1989\n",
      "Household                1548\n",
      "Frozen Foods             1426\n",
      "Dairy                    1136\n",
      "Baking Goods             1086\n",
      "Canned                   1084\n",
      "Health and Hygiene        858\n",
      "Meat                      736\n",
      "Soft Drinks               726\n",
      "Breads                    416\n",
      "Hard Drinks               362\n",
      "Others                    280\n",
      "Starchy Foods             269\n",
      "Breakfast                 186\n",
      "Seafood                    89\n",
      "Name: Item_Type, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable  Outlet_Location_Type\n",
      "Tier 3    5583\n",
      "Tier 2    4641\n",
      "Tier 1    3980\n",
      "Name: Outlet_Location_Type, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable  Outlet_Size\n",
      "Medium    4655\n",
      "Small     3980\n",
      "High      1553\n",
      "Name: Outlet_Size, dtype: int64\n",
      "\n",
      "Frequency of Categories for variable  Outlet_Type\n",
      "Supermarket Type1    9294\n",
      "Grocery Store        1805\n",
      "Supermarket Type3    1559\n",
      "Supermarket Type2    1546\n",
      "Name: Outlet_Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Filter categorical variables\n",
    "\n",
    "categorical_columns= [x for x in data.dtypes.index if data.dtypes[x]=='object']\n",
    "\n",
    "#Exclude ID cols and source:\n",
    "categorical_columns= [x for x in categorical_columns if x not in ['Item_Identifier', 'Outlet_Identifier','source']]\n",
    "\n",
    "#Print frequency of categories\n",
    "for col in categorical_columns:\n",
    "    print('\\nFrequency of Categories for variable ',col)\n",
    "    print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output gives us following observations:\n",
    "1. <b>Item_Fat_Content:</b> Some of 'Low Fat' values mis-coded as 'low fat' and 'LF'.Also, some of 'Regular' are mentioned as 'regular'.\n",
    "2. <b>Item_Type:</b> Not all categories have substantial numbers. It looks like combining them can give better results.\n",
    "3. <b>Outlet_Type:</b> Supermarket Type2 and Type3 can be combined. But we should check if that's a good idea before doing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step typically involves imputing missing values and treating outliers. Though outlier removal is very important in regression techniques, advanced tree based algorithms are impervious to outliers. So I'll leave it to you to try it out. We'll focus on the imputation step here, which is a very important step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Imputing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found two variables with missing values - Item_Weight and Outlet_Size. Lets impute the former by the average weight of the particular item. This can be done as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original #missing:  2439\n",
      "Final #missing:  0\n"
     ]
    }
   ],
   "source": [
    "#Determine the average weight per item:\n",
    "item_avg_weight= data.pivot_table(values='Item_Weight', index='Item_Identifier')\n",
    "\n",
    "#Get a boolean variable specifying missing Item_Weight values\n",
    "miss_bool=data['Item_Weight'].isnull()\n",
    "\n",
    "#Impute data and check #missing values before and after imputation to confirm \n",
    "print(\"Original #missing: \", sum(miss_bool))\n",
    "data.loc[miss_bool,'Item_Weight'] = data.loc[miss_bool,'Item_Identifier'].apply(lambda x: item_avg_weight.loc[x])\n",
    "print(\"Final #missing: \", sum(data['Item_Weight'].isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that the column has no missing values now. Lets impute Outlet_Size with the mode of the Outlet_Size for the particular type of outlet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
